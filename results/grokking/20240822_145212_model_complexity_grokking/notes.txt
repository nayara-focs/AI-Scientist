# Title: Exploring Model Complexity and Grokking: Analyzing the interplay of architecture depth and attention heads on generalization in neural networks
# Experiment description: Extend the existing code to include Transformer models with varying layer depths and attention head configurations. Train these models on diverse mathematical operation datasets to observe how model complexity affects the grokking phenomenon. Evaluate the models on standard performance metrics and analyze the patterns of learning and generalization.
## Run 0: Baseline
Results: {'x_div_y': {'final_train_loss_mean': 0.018480150727555156, 'final_val_loss_mean': 0.02484893426299095, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 0.9998779296875, 'step_val_acc_99_mean': 4080.0}, 'x_minus_y': {'final_train_loss_mean': 0.0059833748964592814, 'final_val_loss_mean': 0.023401675280183554, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4515.0}, 'x_plus_y': {'final_train_loss_mean': 0.0031202899990603328, 'final_val_loss_mean': 0.0034879110753536224, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 2410.0}, 'permutation': {'final_train_loss_mean': 0.15331691037863493, 'final_val_loss_mean': 7.519349098205566, 'final_train_acc_mean': 0.969531238079071, 'final_val_acc_mean': 0.013427734375, 'step_val_acc_99_mean': 5000.0}}
Description: Baseline results.
