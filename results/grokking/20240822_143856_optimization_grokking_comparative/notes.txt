# Title: Comparative Optimization Grokking: Investigating the impact of SGD, Adam, and RMSprop on grokking in overparametrized neural networks
# Experiment description: Compare the effects of SGD, Adam, and RMSprop on the grokking phenomenon in neural networks trained on small algorithmically generated datasets. Analyze how different optimization algorithms influence generalization performance, speed of learning, and the occurrence of grokking.
## Run 0: Baseline
Results: {'x_div_y': {'final_train_loss_mean': 0.018480150727555156, 'final_val_loss_mean': 0.02484893426299095, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 0.9998779296875, 'step_val_acc_99_mean': 4080.0}, 'x_minus_y': {'final_train_loss_mean': 0.0059833748964592814, 'final_val_loss_mean': 0.023401675280183554, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 4515.0}, 'x_plus_y': {'final_train_loss_mean': 0.0031202899990603328, 'final_val_loss_mean': 0.0034879110753536224, 'final_train_acc_mean': 1.0, 'final_val_acc_mean': 1.0, 'step_val_acc_99_mean': 2410.0}, 'permutation': {'final_train_loss_mean': 0.15331691037863493, 'final_val_loss_mean': 7.519349098205566, 'final_train_acc_mean': 0.969531238079071, 'final_val_acc_mean': 0.013427734375, 'step_val_acc_99_mean': 5000.0}}
Description: Baseline results.
